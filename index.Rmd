---
title: "Bayesian variable selection with logistic prior inclusion probability"
author: "Yifan Zhou"
date: "12/1/2018"
output: 
  html_document: 
    toc: true
    toc_float: true
---

<span style="color:black; font-size:1.4em;">The goal of this project is to reconstruct cell-type specific gene regulatory networks using NGS data.

## Model setup

Suppose there are _P_ enhancers (or ATAC-seq peaks) in the vicinity of a gene of interest _G_.    
Let $x_j$ be chromatin accessibility of enhancer _j_ and $\beta_j$ be its effect on gene _G_, let _y_ be the expression or promoter activity of gene _G_. We have:
$$y=\sum_{j=1}^P \beta_j x_j + \epsilon , \hspace{5mm} \epsilon \sim N(0,\sigma^2_e)$$
where $\beta_j \sim f(A_j)$, $A_j$ here are features in enhancer _j_ (_e.g._ Hi-C contact with gene promoter, TF footprint strength, ...).   
One choice is a spike-and-slab prior on $\beta_j$ with logistic inclusion probability:
$$\beta_j \sim \pi_j N(0,\sigma^2_{\beta}) + (1-\pi_j)\delta_j , \hspace{5mm} \text{log }\frac{\pi_j}{1-\pi_j}=\alpha A_j$$
If we introduce $\gamma_j$ as a latent indicator variable that takes on the value 1 when $\beta_j$ is non-zero, and 0 when $\beta_j$ is zero, then the prior model is equivalent to:
$$\beta_j | \gamma_j=0 \sim \delta_0, \hspace{5mm}  \beta_j | \gamma_j=1 \sim N(0,\sigma^2_{\beta}), $$
$$\text{and}  \hspace{5mm}  \text{log }\frac{p(\gamma_j=1)}{p(\gamma_j=0)}=\alpha A_j .$$
Suppose the sample size is _N_, the joint probability density function is:
$$
\begin{align*}
&\text{P}(\boldsymbol y, \boldsymbol \beta, \boldsymbol\gamma, \alpha | \boldsymbol X, \boldsymbol A, \sigma^2_e, \sigma^2_{\beta} )\\ 
&= \prod_{i=1}^N \text{P}(y_i|X_{i\cdot},\boldsymbol \beta,\sigma^2_e) \prod_{j=1}^P \text{P}(\beta_j|\gamma_j,\sigma^2_{\beta}) \prod_{j=1}^P \text{P}(\gamma_j|\alpha,A_j)\\
&= \prod_{i=1}^N N(y_i;\sum_{j=1}^P \beta_j x_{ij},\sigma^2_e) \prod_{j=1}^P [N(\beta_j;0,\sigma^2_{\beta})^{\gamma_j } \delta_0(\beta_j)^{1-\gamma_j}] \prod_{j=1}^P (1+e^{-\alpha A_j})^{-\gamma_j}(1+e^{\alpha A_j})^{\gamma_j-1}
\end{align*}
$$

## Gibbs sampling algorithm

We sample each of the parameters based on their posterior distributions:

* Sample $\boldsymbol \gamma, \boldsymbol \beta$ given $\boldsymbol y, \boldsymbol X, \alpha, \boldsymbol A, \sigma^2_e, \sigma^2_{\beta}$ :   
For $j=1,2, ... ,P$:    
First, sample $\gamma_j$ according to: 
$$\frac{p(\gamma_j=1|\cdot)}{p(\gamma_j=0|\cdot)} =\sqrt{\frac{\lambda}{\sigma^2_{\beta}}} \text{ exp}\Bigl(\frac{\nu^2}{2\lambda}\Bigr) \text{ exp}(\alpha A_j).$$
Next, if $\gamma_j=1$, sample $\beta_j \sim N(\nu, \lambda)$; otherwise, $\beta_j=0$.   
Here, $\nu = \lambda \cdot \frac{1}{\sigma^2_e} \sum_{i=1}^N x_{ij}(y_i-\sum_{k\neq j}x_{ik}\beta_k)$,
$\lambda = \Bigl(\frac{\sum_{i=1}^N x_{ij}^2}{\sigma^2_e}+\frac{1}{\sigma^2_{\beta}}\Bigr)^{-1}$.

* Sample $\sigma^2_e$

* Sample $\sigma^2_{\beta}$

* Update $\alpha$ using an EM-like approach:   
Suppose that at step $(t)$, the posterior probability $p(\gamma|\cdot)=q^{(t)}(\gamma)$, then $\alpha^{(t)}$ can be estimated as:
$$
\begin{align*}
\alpha^{(t)} &= \underset{\alpha}{\text{argmax }} \mathbb{E}_{\gamma \sim q^{(t)}} \text{ log } p(\gamma|\alpha,A)\\
&= \underset{\alpha}{\text{argmax }} \mathbb{E}_{\gamma \sim q^{(t)}} \Bigl[ \sum_{j=1}^P -\gamma_j \text{ log}(1+e^{-\alpha_0-\alpha_1 A_j}) - (1-\gamma_j) \text{ log}(1+e^{\alpha_0+\alpha_1 A_j})\Bigr]\\
&= \underset{\alpha}{\text{argmax }} \sum_{j=1}^P \Bigl[q^{(t)}(\gamma_j=1)(\alpha_0+\alpha_1 A_j) - \text{log}(1+e^{\alpha_0+\alpha_1 A_j})\Bigr]
\end{align*}
$$

